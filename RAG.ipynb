{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtNI3RNzkoFnkyTcbflByG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbishtabhishek/RAG-Implementation/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the needed modules\n",
        "!pip install langchain-google-genai google-ai-generativelanguage google-generativeai\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain_community\n",
        "!pip install langchain_text_splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_LmAV7dp8Vj",
        "outputId": "09c41c18-5734-4bd0-e88f-8ed30adecf99",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.62 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.63)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (2.25.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage) (5.29.5)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage) (4.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage) (2025.4.26)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m357.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.60 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.63)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (2.0.2)\n",
            "Collecting chromadb>=1.0.9 (from langchain_chroma)\n",
            "  Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (2.11.5)\n",
            "Collecting fastapi==0.115.9 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.34.3)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading posthog-4.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.72.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.24.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (0.3.44)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.60->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.3.60->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.3.60->langchain_chroma) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (0.32.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.4-py3-none-any.whl (11 kB)\n",
            "Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-4.10.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3f581b4aafeceb922f6caa06d47905f150731a8ff9fd9ed5a0c3e926ddd37804\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain_chroma\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.12 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain_chroma-0.2.4 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 posthog-4.10.0 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.65 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.44)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.5)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langsmith, dataclasses-json, langchain-core, langchain_community\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.44\n",
            "    Uninstalling langsmith-0.3.44:\n",
            "      Successfully uninstalled langsmith-0.3.44\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.63\n",
            "    Uninstalling langchain-core-0.3.63:\n",
            "      Successfully uninstalled langchain-core-0.3.63\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-core-0.3.65 langchain_community-0.3.25 langsmith-0.3.45 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_text_splitters) (0.3.65)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (2.11.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain_text_splitters) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import bs4\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "I_wue87VjGfd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"GoogleAPIkey\") #my google API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"LangchainAPIkey\")  #my langchain API key\n",
        "\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG\"\n",
        "os.environ[\"USER_AGENT\"]=\"langchain-rag-app/0.1\"\n"
      ],
      "metadata": {
        "id": "3NgPaUq9jLx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71d4f36-e59b-44cf-9ee5-8a2bc991dd5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GoogleAPIkey··········\n",
            "LangchainAPIkey··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using google generative AI for Chat and Embiddings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\", # this model is used to convert texts into vectors for search.\n",
        ")\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model = \"gemma-3-1b-it\", convert_system_message_to_human=True)\n",
        "print(model.invoke(\"hello\").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaLaM9MZjOB9",
        "outputId": "8d3ad021-d5a3-47f4-9eb7-4a178e4abd54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there! How's your day going so far? 😊 \n",
            "\n",
            "Is there anything you'd like to chat about or need help with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the libraries to be used\n",
        "\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate #used to generate promp\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "Cd6f3-tOjR97"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the Web Base Loader to load the desired webpage\n",
        "\n",
        "loader=WebBaseLoader(\n",
        "    web_path=(\"https://lilianweng.github.io/posts/2019-06-23-meta-rl/\",),\n",
        "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\",\"post-title\",\"post-header\"))),\n",
        ")\n",
        "\n",
        "doc=loader.load()\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ftFKcZrjWO8",
        "outputId": "5fe03d9d-5164-4da9-c357-0e82808ed386",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='\\n\\n      Meta Reinforcement Learning\\n    \\nDate: June 23, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng\\n\\n\\n\\nIn my earlier post on meta-learning, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to “meta-learn” Reinforcement Learning (RL) tasks by developing an agent that can solve unseen tasks fast and efficiently.\\nTo recap, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training. The adaptation process, essentially a mini learning session, happens at test with limited exposure to the new configurations. Even without any explicit fine-tuning (no gradient backpropagation on trainable variables), the meta-learning model autonomously adjusts internal hidden states to learn.\\nTraining RL algorithms can be notoriously difficult sometimes. If the meta-learning agent could become so smart that the distribution of solvable unseen tasks grows extremely broad, we are on track towards general purpose methods — essentially building a “brain” which would solve all kinds of RL problems without much human interference or manual feature engineering. Sounds amazing, right? \\uf8ffüíñ\\nOn the Origin of Meta-RL#\\nBack in 2001#\\nI encountered a paper  written in 2001 by Hochreiter et al. when reading Wang et al., 2016. Although the idea was proposed for supervised learning, there are so many resemblances to the current approach to meta-RL.\\n\\n\\nThe meta-learning system consists of the supervisory and the subordinate systems. The subordinate system is a recurrent neural network that takes as input both the observation at the current time step, $x\\\\_t$ and the label at the last time step, $y\\\\_{t-1}$. (Image source: Hochreiter et al., 2001)\\n\\nHochreiter’s meta-learning model is a recurrent network with LSTM cell. LSTM is a good choice because it can internalize a history of inputs and tune its own weights effectively through BPTT. The training data contains $K$ sequences and each sequence is consist of $N$ samples generated by a target function $f_k(.), k=1, \\\\dots, K$,\\n\\n$$\\n\\\\{\\\\text{input: }(\\\\mathbf{x}^k_i, \\\\mathbf{y}^k_{i-1}) \\\\to \\\\text{label: }\\\\mathbf{y}^k_i\\\\}_{i=1}^N\\n\\\\text{ where }\\\\mathbf{y}^k_i = f_k(\\\\mathbf{x}^k_i)\\n$$ \\n\\nNoted that the last label $\\\\mathbf{y}^k_{i-1}$ is also provided as an auxiliary input so that the function can learn the presented mapping.\\nIn the experiment of decoding two-dimensional quadratic functions, $a x_1^2 + b x_2^2 + c x_1 x_2 + d x_1 + e x_2 + f$, with coefficients $a$-$f$ are randomly sampled from [-1, 1], this meta-learning system was able to approximate the function after seeing only ~35 examples.\\nProposal in 2016#\\nIn the modern days of DL, Wang et al. (2016) and Duan et al. (2017) simultaneously proposed the very similar idea of Meta-RL (it is called RL^2 in the second paper). A meta-RL model is trained over a distribution of MDPs, and at test time, it is able to learn to solve a new task quickly. The goal of meta-RL is ambitious, taking one step further towards general algorithms.\\nDefine Meta-RL#\\nMeta Reinforcement Learning, in short, is to do meta-learning in the field of reinforcement learning. Usually the train and test tasks are different but drawn from the same family of problems; i.e., experiments in the papers included multi-armed bandit with different reward probabilities, mazes with different layouts, same robots but with different physical parameters in simulator, and many others.\\nFormulation#\\nLet’s say we have a distribution of tasks, each formularized as an MDP (Markov Decision Process), $M_i \\\\in \\\\mathcal{M}$. An MDP is determined by a 4-tuple, $M_i= \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P_i, R_i \\\\rangle$:\\n\\n\\n\\nSymbol\\nMeaning\\n\\n\\n\\n\\n$\\\\mathcal{S}$\\nA set of states.\\n\\n\\n$\\\\mathcal{A}$\\nA set of actions.\\n\\n\\n$P_i: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{S} \\\\to \\\\mathbb{R}_{+}$\\nTransition probability function.\\n\\n\\n$R_i: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}$\\nReward function.\\n\\n\\n\\n(RL^2 paper adds an extra parameter, horizon $T$, into the MDP tuple to emphasize that each MDP should have a finite horizon.)\\nNote that common state $\\\\mathcal{S}$ and action space $\\\\mathcal{A}$ are used above, so that a (stochastic) policy: $\\\\pi_\\\\theta: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}_{+}$ would get inputs compatible across different tasks. The test tasks are sampled from the same distribution $\\\\mathcal{M}$ or slightly modified version.\\n\\n\\nIllustration of meta-RL, containing two optimization loops. The outer loop samples a new environment in every iteration and adjusts parameters that determine the agent\\'s behavior. In the inner loop, the agent interacts with the environment and optimizes for the maximal reward. (Image source: Botvinick, et al. 2019)\\n\\nMain Differences from RL#\\nThe overall configure of meta-RL is very similar to an ordinary RL algorithm, except that the last reward $r_{t-1}$ and the last action $a_{t-1}$ are also incorporated into the policy observation in addition to the current state $s_t$.\\n\\nIn RL: $\\\\pi_\\\\theta(s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\\nIn meta-RL: $\\\\pi_\\\\theta(a_{t-1}, r_{t-1}, s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\\n\\nThe intention of this design is to feed a history into the model so that the policy can internalize the dynamics between states, rewards, and actions in the current MDP and adjust its strategy accordingly. This is well aligned with the setup in Hochreiter’s system. Both meta-RL and RL^2 implemented an LSTM policy and the LSTM’s hidden states serve as a memory for tracking characteristics of the trajectories. Because the policy is recurrent, there is no need to feed the last state as inputs explicitly.\\nThe training procedure works as follows:\\n\\nSample a new MDP, $M_i \\\\sim \\\\mathcal{M}$;\\nReset the hidden state of the model;\\nCollect multiple trajectories and update the model weights;\\nRepeat from step 1.\\n\\n\\n\\nIn the meta-RL paper, different actor-critic architectures all use a recurrent model. Last reward and last action are additional inputs. The observation is fed into the LSTM either as a one-hot vector or as an embedding vector after passed through an encoder model. (Image source: Wang et al., 2016)\\n\\n\\n\\nAs described in the RL^2 paper, illustration of the procedure of the model interacting with a series of MDPs in training time . (Image source: Duan et al., 2017)\\n\\nKey Components#\\nThere are three key components in Meta-RL:\\n\\n‚≠ê A Model with Memory\\n\\nA recurrent neural network maintains a hidden state. Thus, it could acquire and memorize the knowledge about the current task by updating the hidden state during rollouts. Without memory, meta-RL would not work.\\n\\n\\n‚≠ê Meta-learning Algorithm\\n\\nA meta-learning algorithm refers to how we can update the model weights to optimize for the purpose of solving an unseen task fast at test time. In both Meta-RL and RL^2 papers, the meta-learning algorithm is the ordinary gradient descent update of LSTM with hidden state reset between a switch of MDPs.\\n\\n\\n‚≠ê A Distribution of MDPs\\n\\nWhile the agent is exposed to a variety of environments and tasks during training, it has to learn how to adapt to different MDPs.\\n\\nAccording to Botvinick et al. (2019), one source of slowness in RL training is weak inductive bias ( = “a set of assumptions that the learner uses to predict outputs given inputs that it has not encountered”). As a general ML rule, a learning algorithm with weak inductive bias will be able to master a wider range of variance, but usually, will be less sample-efficient. Therefore, to narrow down the hypotheses with stronger inductive biases help improve the learning speed.\\nIn meta-RL, we impose certain types of inductive biases from the task distribution and store them in memory. Which inductive bias to adopt at test time depends on the algorithm. Together, these three key components depict a compelling view of meta-RL: Adjusting the weights of a recurrent network is slow but it allows the model to work out a new task fast with its own RL algorithm implemented in its internal activity dynamics.\\nMeta-RL interestingly and not very surprisingly matches the ideas in the AI-GAs (“AI-Generating Algorithms”) paper by Jeff Clune (2019). He proposed that one efficient way towards building general AI is to make learning as automatic as possible. The AI-GAs approach involves three pillars: (1) meta-learning architectures, (2) meta-learning algorithms, and (3) automatically generated environments for effective learning.\\n\\nThe topic of designing good recurrent network architectures is a bit too broad to be discussed here, so I will skip it. Next, let’s look further into another two components: meta-learning algorithms in the context of meta-RL and how to acquire a variety of training MDPs.\\nMeta-Learning Algorithms for Meta-RL#\\nMy previous post on meta-learning has covered several classic meta-learning algorithms. Here I’m gonna include more related to RL.\\nOptimizing Model Weights for Meta-learning#\\nBoth MAML (Finn, et al. 2017) and Reptile (Nichol et al., 2018) are methods on updating model parameters in order to achieve good generalization performance on new tasks. See an earlier post section on MAML and Reptile.\\nMeta-learning Hyperparameters#\\nThe return function in an RL problem, $G_t^{(n)}$ or $G_t^\\\\lambda$, involves a few hyperparameters that are often set heuristically, like the discount factor $\\\\gamma$ and the bootstrapping parameter $\\\\lambda$.\\nMeta-gradient RL (Xu et al., 2018) considers them as meta-parameters, $\\\\eta=\\\\{\\\\gamma, \\\\lambda \\\\}$, that can be tuned and learned online while an agent is interacting with the environment. Therefore, the return becomes a function of $\\\\eta$ and dynamically adapts itself to a specific task over time.\\n\\n$$\\n\\\\begin{aligned}\\nG_\\\\eta^{(n)}(\\\\tau_t) &= R_{t+1} + \\\\gamma R_{t+2} + \\\\dots + \\\\gamma^{n-1}R_{t+n} + \\\\gamma^n v_\\\\theta(s_{t+n}) & \\\\scriptstyle{\\\\text{; n-step return}} \\\\\\\\\\nG_\\\\eta^{\\\\lambda}(\\\\tau_t) &= (1-\\\\lambda) \\\\sum_{n=1}^\\\\infty \\\\lambda^{n-1} G_\\\\eta^{(n)} & \\\\scriptstyle{\\\\text{; Œª-return, mixture of n-step returns}}\\n\\\\end{aligned}\\n$$\\n\\nDuring training, we would like to update the policy parameters with gradients as a function of all the information in hand, $\\\\theta’ = \\\\theta + f(\\\\tau, \\\\theta, \\\\eta)$, where $\\\\theta$ are the current model weights, $\\\\tau$ is a sequence of trajectories, and $\\\\eta$ are the meta-parameters.\\nMeanwhile, let’s say we have a meta-objective function $J(\\\\tau, \\\\theta, \\\\eta)$ as a performance measure. The training process follows the principle of online cross-validation, using a sequence of consecutive experiences:\\n\\nStarting with parameter $\\\\theta$, the policy $\\\\pi_\\\\theta$ is updated on the first batch of samples $\\\\tau$, resulting in $\\\\theta’$.\\nThen we continue running the policy $\\\\pi_{\\\\theta’}$ to collect a new set of experiences $\\\\tau’$, just following $\\\\tau$ consecutively in time. The performance is measured as $J(\\\\tau’, \\\\theta’, \\\\bar{\\\\eta})$ with a fixed meta-parameter $\\\\bar{\\\\eta}$.\\nThe gradient of meta-objective $J(\\\\tau’, \\\\theta’, \\\\bar{\\\\eta})$ w.r.t. $\\\\eta$ is used to update $\\\\eta$:\\n\\n\\n$$\\n\\\\begin{aligned}\\n\\\\Delta \\\\eta\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\eta} \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\theta\\'} \\\\frac{d\\\\theta\\'}{d\\\\eta} & \\\\scriptstyle{\\\\text{ ; single variable chain rule.}} \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\theta\\'} \\\\frac{\\\\partial (\\\\theta + f(\\\\tau, \\\\theta, \\\\eta))}{\\\\partial\\\\eta}  \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\theta\\'} \\\\Big(\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\theta}\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\\\frac{d\\\\eta}{d\\\\eta} \\\\Big) & \\\\scriptstyle{\\\\text{; multivariable chain rule.}}\\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\theta\\'} \\\\Big( \\\\color{red}{\\\\big(\\\\mathbf{I} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\theta}\\\\big)}\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\\\Big) & \\\\scriptstyle{\\\\text{; secondary gradient term in red.}}\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\beta$ is the learning rate for $\\\\eta$.\\nThe meta-gradient RL algorithm simplifies the computation by setting the secondary gradient term to zero, $\\\\mathbf{I} + \\\\partial g(\\\\tau, \\\\theta, \\\\eta)/\\\\partial\\\\theta = 0$ — this choice prefers the immediate effect of the meta-parameters $\\\\eta$ on the parameters $\\\\theta$. Eventually we get:\\n\\n$$\\n\\\\Delta \\\\eta = -\\\\beta \\\\frac{\\\\partial J(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta})}{\\\\partial \\\\theta\\'} \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\n$$\\n\\nExperiments in the paper adopted the meta-objective function same as $TD(\\\\lambda)$ algorithm, minimizing the error between the approximated value function $v_\\\\theta(s)$ and the $\\\\lambda$-return:\\n\\n$$\\n\\\\begin{aligned}\\nJ(\\\\tau, \\\\theta, \\\\eta) &= (G^\\\\lambda_\\\\eta(\\\\tau) - v_\\\\theta(s))^2 \\\\\\\\\\nJ(\\\\tau\\', \\\\theta\\', \\\\bar{\\\\eta}) &= (G^\\\\lambda_{\\\\bar{\\\\eta}}(\\\\tau\\') - v_{\\\\theta\\'}(s\\'))^2\\n\\\\end{aligned}\\n$$\\n\\nMeta-learning the Loss Function#\\nIn policy gradient algorithms, the expected total reward is maximized by updating the policy parameters $\\\\theta$ in the direction of estimated gradient (Schulman et al., 2016),\\n\\n$$\\ng = \\\\mathbb{E}[\\\\sum_{t=0}^\\\\infty \\\\Psi_t \\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta (a_t \\\\mid s_t)]\\n$$\\n\\nwhere the candidates for $\\\\Psi_t$ include the trajectory return $G_t$, the Q value $Q(s_t, a_t)$, or the advantage value $A(s_t, a_t)$. The corresponding surrogate loss function for the policy gradient can be reverse-engineered:\\n\\n$$\\nL_\\\\text{pg} = \\\\mathbb{E}[\\\\sum_{t=0}^\\\\infty \\\\Psi_t \\\\log \\\\pi_\\\\theta (a_t \\\\mid s_t)]\\n$$\\n\\nThis loss function is a measure over a history of trajectories, $(s_0, a_0, r_0, \\\\dots, s_t, a_t, r_t, \\\\dots)$. Evolved Policy Gradient (EPG; Houthooft, et al, 2018) takes a step further by defining the policy gradient loss function as a temporal convolution (1-D convolution) over the agent’s past experience, $L_\\\\phi$. The parameters $\\\\phi$ of the loss function network are evolved in a way that an agent can achieve higher returns.\\nSimilar to many meta-learning algorithms, EPG has two optimization loops:\\n\\nIn the internal loop, an agent learns to improve its policy $\\\\pi_\\\\theta$.\\nIn the outer loop, the model updates the parameters $\\\\phi$ of the loss function $L_\\\\phi$. Because there is no explicit way to write down a differentiable equation between the return and the loss, EPG turned to Evolutionary Strategies (ES).\\n\\nA general idea is to train a population of $N$ agents, each of them is trained with the loss function $L_{\\\\phi + \\\\sigma \\\\epsilon_i}$ parameterized with $\\\\phi$ added with a small Gaussian noise $\\\\epsilon_i \\\\sim \\\\mathcal{N}(0, \\\\mathbf{I})$ of standard deviation $\\\\sigma$. During the inner loop’s training, EPG tracks a history of experience and updates the policy parameters according to the loss function $L_{\\\\phi + \\\\sigma\\\\epsilon_i}$ for each agent:\\n\\n$$\\n\\\\theta_i \\\\leftarrow \\\\theta - \\\\alpha_\\\\text{in} \\\\nabla_\\\\theta L_{\\\\phi + \\\\sigma \\\\epsilon_i} (\\\\pi_\\\\theta, \\\\tau_{t-K, \\\\dots, t})\\n$$\\n\\nwhere $\\\\alpha_\\\\text{in}$ is the learning rate of the inner loop and $\\\\tau_{t-K, \\\\dots, t}$ is a sequence of $M$ transitions up to the current time step $t$.\\nOnce the inner loop policy is mature enough, the policy is evaluated by the mean return $\\\\bar{G}_{\\\\phi+\\\\sigma\\\\epsilon_i}$ over multiple randomly sampled trajectories. Eventually, we are able to estimate the gradient of $\\\\phi$ according to NES numerically (Salimans et al, 2017). While repeating this process, both the policy parameters $\\\\theta$ and the loss function weights $\\\\phi$ are being updated simultaneously to achieve higher returns.\\n\\n$$\\n\\\\phi \\\\leftarrow \\\\phi + \\\\alpha_\\\\text{out} \\\\frac{1}{\\\\sigma N} \\\\sum_{i=1}^N \\\\epsilon_i G_{\\\\phi+\\\\sigma\\\\epsilon_i}\\n$$\\n\\nwhere $\\\\alpha_\\\\text{out}$ is the learning rate of the outer loop.\\nIn practice, the loss $L_\\\\phi$ is bootstrapped with an ordinary policy gradient (such as REINFORCE or PPO) surrogate loss $L_\\\\text{pg}$, $\\\\hat{L} = (1-\\\\alpha) L_\\\\phi + \\\\alpha L_\\\\text{pg}$. The weight $\\\\alpha$ is annealing from 1 to 0 gradually during training. At test time, the loss function parameter $\\\\phi$ stays fixed and the loss value is computed over a history of experience to update the policy parameters $\\\\theta$.\\nMeta-learning the Exploration Strategies#\\nThe exploitation vs exploration dilemma is a critical problem in RL. Common ways to do exploration include $\\\\epsilon$-greedy, random noise on actions, or stochastic policy with built-in randomness on the action space.\\nMAESN (Gupta et al, 2018) is an algorithm to learn structured action noise from prior experience for better and more effective exploration. Simply adding random noise on actions cannot capture task-dependent or time-correlated exploration strategies. MAESN changes the policy to condition on a per-task random variable $z_i \\\\sim \\\\mathcal{N}(\\\\mu_i, \\\\sigma_i)$, for $i$-th task $M_i$, so we would have a policy $a \\\\sim \\\\pi_\\\\theta(a\\\\mid s, z_i)$.\\nThe latent variable $z_i$ is sampled once and fixed during one episode. Intuitively, the latent variable determines one type of behavior (or skills) that should be explored more at the beginning of a rollout and the agent would adjust its actions accordingly. Both the policy parameters and latent space are optimized to maximize the total task rewards. In the meantime, the policy learns to make use of the latent variables for exploration.\\nIn addition,  the loss function includes a KL divergence between the learned latent variable and a unit Gaussian prior, $D_\\\\text{KL}(\\\\mathcal{N}(\\\\mu_i, \\\\sigma_i)|\\\\mathcal{N}(0, \\\\mathbf{I}))$. On one hand, it restricts the learned latent space not too far from a common prior. On the other hand, it creates the variational evidence lower bound (ELBO) for the reward function. Interestingly the paper found that $(\\\\mu_i, \\\\sigma_i)$ for each task are usually close to the prior at convergence.\\n\\n\\nThe policy is conditioned on a latent variable variable $z\\\\_i \\\\sim \\\\mathcal{N}(\\\\mu, \\\\sigma)$ that is sampled once every episode. Each task has different hyperparameters for the latent variable distribution, $(\\\\mu\\\\_i, \\\\sigma\\\\_i)$ and they are optimized in the outer loop. (Image source: Gupta et al, 2018)\\n\\nEpisodic Control#\\nA major criticism of RL is on its sample inefficiency. A large number of samples and small learning steps are required for incremental parameter adjustment in RL in order to maximize generalization and avoid catastrophic forgetting of earlier learning (Botvinick et al., 2019).\\nEpisodic control (Lengyel & Dayan, 2008) is proposed as a solution to avoid forgetting and improve generalization while training at a faster speed. It is partially inspired by hypotheses on instance-based hippocampal learning.\\nAn episodic memory keeps explicit records of past events and uses these records directly as point of reference for making new decisions (i.e. just like metric-based meta-learning). In MFEC (Model-Free Episodic Control; Blundell et al., 2016), the memory is modeled as a big table, storing the state-action pair $(s, a)$ as key and the corresponding Q-value $Q_\\\\text{EC}(s, a)$ as value. When receiving a new observation $s$, the Q value is estimated in an non-parametric way as the average Q-value of top $k$ most similar samples:\\n\\n$$\\n\\\\hat{Q}_\\\\text{EC}(s, a) = \\n\\\\begin{cases}\\nQ_\\\\text{EC}(s, a)                      & \\\\text{if } (s,a) \\\\in Q_\\\\text{EC}, \\\\\\\\\\n\\\\frac{1}{k} \\\\sum_{i=1}^k Q(s^{(i)}, a) & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\nwhere $s^{(i)}, i=1, \\\\dots, k$ are top $k$ states with smallest distances to the state $s$. Then the action that yields the highest estimated Q value is selected. Then the memory table is updated according to the return received at $s_t$:\\n\\n$$\\nQ_\\\\text{EC}(s, a) \\\\leftarrow\\n\\\\begin{cases}\\n\\\\max\\\\{Q_\\\\text{EC}(s_t, a_t), G_t\\\\}  & \\\\text{if } (s,a) \\\\in Q_\\\\text{EC}, \\\\\\\\\\nG_t                                 & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\nAs a tabular RL method, MFEC suffers from large memory consumption and a lack of ways to generalize among similar states. The first one can be fixed with an LRU cache. Inspired by metric-based meta-learning, especially Matching Networks (Vinyals et al., 2016), the generalization problem is improved in a follow-up algorithm, NEC (Neural Episodic Control; Pritzel et al., 2016).\\nThe episodic memory in NEC is a Differentiable Neural Dictionary (DND), where the key is a convolutional embedding vector of input image pixels and the value stores estimated Q value. Given an inquiry key, the output is a weighted sum of values of top similar keys, where the weight is a normalized kernel measure between the query key and the selected key in the dictionary. This sounds like a hard attention machanism.\\n\\n\\nFig. 6 Illustrations of episodic memory module in NEC and two operations on a differentiable neural dictionary. (Image source: Pritzel et al., 2016)\\n\\nFurther, Episodic LSTM (Ritter et al., 2018) enhances the basic LSTM architecture with a DND episodic memory, which stores task context embeddings as keys and the LSTM cell states as values. The stored hidden states are retrieved and added directly to the current cell state through the same gating mechanism within LSTM:\\n\\n\\nIllustration of the episodic LSTM architecture. The additional structure of episodic memory is in bold. (Image source: Ritter et al., 2018)\\n\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{c}_t &= \\\\mathbf{i}_t \\\\circ \\\\mathbf{c}_\\\\text{in} + \\\\mathbf{f}_t \\\\circ \\\\mathbf{c}_{t-1} + \\\\color{green}{\\\\mathbf{r}_t \\\\circ \\\\mathbf{c}_\\\\text{ep}} &\\\\\\\\\\n\\\\mathbf{i}_t &= \\\\sigma(\\\\mathbf{W}_{i} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_i) & \\\\scriptstyle{\\\\text{; input gate}} \\\\\\\\\\n\\\\mathbf{f}_t &= \\\\sigma(\\\\mathbf{W}_{f} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_f) & \\\\scriptstyle{\\\\text{; forget gate}} \\\\\\\\\\n\\\\color{green}{\\\\mathbf{r}_t} & \\\\color{green}{=} \\\\color{green}{\\\\sigma(\\\\mathbf{W}_{r} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_r)} & \\\\scriptstyle{\\\\text{; reinstatement gate}}\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\mathbf{c}_t$ and $\\\\mathbf{h}_t$ are hidden and cell state at time $t$; $\\\\mathbf{i}_t$, $\\\\mathbf{f}_t$ and $\\\\mathbf{r}_t$ are input, forget and reinstatement gates, respectively; $\\\\mathbf{c}_\\\\text{ep}$ is the retrieved cell state from episodic memory. The newly added episodic memory components are marked in green.\\nThis architecture provides a shortcut to the prior experience through context-based retrieval. Meanwhile, explicitly saving the task-dependent experience in an external memory avoids forgetting. In the paper, all the experiments have manually designed context vectors. How to construct an effective and efficient format of task context embeddings for more free-formed tasks would be an interesting topic.\\nOverall the capacity of episodic control is limited by the complexity of the environment. It is very rare for an agent to repeatedly visit exactly the same states in a real-world task, so properly encoding the states is critical. The learned embedding space compresses the observation data into a lower dimension space and, in the meantime, two states being close in this space are expected to demand similar strategies.\\nTraining Task Acquisition#\\nAmong three key components, how to design a proper distribution of tasks is the less studied and probably the most specific one to meta-RL itself. As described above, each task is a MDP: $M_i = \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P_i, R_i \\\\rangle \\\\in \\\\mathcal{M}$. We can build a distribution of MDPs by modifying:\\n\\nThe reward configuration: Among different tasks, same behavior might get rewarded differently according to $R_i$.\\nOr, the environment: The transition function $P_i$ can be reshaped by initializing the environment with varying shifts between states.\\n\\nTask Generation by Domain Randomization#\\nRandomizing parameters in a simulator is an easy way to obtain tasks with modified transition functions. If interested in learning further, check my last post on domain randomization.\\nEvolutionary Algorithm on Environment Generation#\\nEvolutionary algorithm is a gradient-free heuristic-based optimization method, inspired by natural selection. A population of solutions follows a loop of evaluation, selection, reproduction, and mutation. Eventually, good solutions survive and thus get selected.\\nPOET (Wang et al, 2019), a framework based on the evolutionary algorithm, attempts to generate tasks while the problems themselves are being solved. The implementation of POET is only specifically designed for a simple 2D bipedal walker environment but points out an interesting direction. It is noteworthy that the evolutionary algorithm has had some compelling applications in Deep Learning like EPG and PBT (Population-Based Training;  Jaderberg et al, 2017).\\n\\n\\nAn example bipedal walking environment (top) and an overview of POET (bottom). (Image source: POET blog post)\\n\\nThe 2D bipedal walking environment is evolving: from a simple flat surface to a much more difficult trail with potential gaps, stumps, and rough terrains. POET pairs the generation of environmental challenges and the optimization of agents together so as to (a) select agents that can resolve current challenges and (b) evolve environments to be solvable. The algorithm maintains a list of environment-agent pairs and repeats the following:\\n\\nMutation: Generate new environments from currently active environments. Note that here types of mutation operations are created just for bipedal walker and a new environment would demand a new set of configurations.\\nOptimization: Train paired agents within their respective environments.\\nSelection: Periodically attempt to transfer current agents from one environment to another. Copy and update the best performing agent for every environment. The intuition is that skills learned in one environment might be helpful for a different environment.\\n\\nThe procedure above is quite similar to PBT, but PBT mutates and evolves hyperparameters instead. To some extent, POET is doing domain randomization, as all the gaps, stumps and terrain roughness are controlled by some randomization probability parameters. Different from DR, the agents are not exposed to a fully randomized difficult environment all at once, but instead they are learning gradually with a curriculum configured by the evolutionary algorithm.\\nLearning with Random Rewards#\\nAn MDP without a reward function $R$ is known as a Controlled Markov process (CMP). Given a predefined CMP, $\\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P\\\\rangle$, we can acquire a variety of tasks by generating a collection of reward functions $\\\\mathcal{R}$ that encourage the training of an effective meta-learning policy.\\nGupta et al. (2018) proposed two unsupervised approaches  for growing the task distribution in the context of CMP. Assuming there is an underlying latent variable $z \\\\sim p(z)$ associated with every task, it parameterizes/determines a reward function: $r_z(s) = \\\\log D(z|s)$, where a “discriminator” function $D(.)$ is used to extract the latent variable from the state. The paper described two ways to construct a discriminator function:\\n\\nSample random weights $\\\\phi_\\\\text{rand}$ of the discriminator, $D_{\\\\phi_\\\\text{rand}}(z \\\\mid s)$.\\nLearn a discriminator function to encourage diversity-driven exploration. This method is introduced in more details in another sister paper “DIAYN” (Eysenbach et al., 2018).\\n\\nDIAYN, short for “Diversity is all you need”, is a framework to encourage a policy to learn useful skills without a reward function. It explicitly models the latent variable $z$ as a skill embedding and makes the policy conditioned on $z$ in addition to state $s$, $\\\\pi_\\\\theta(a \\\\mid s, z)$. (Ok, this part is same as MAESN unsurprisingly, as the papers are from the same group.) The design of DIAYN is motivated by a few hypotheses:\\n\\nSkills should be diverse and lead to visitations of different states. ‚Üí maximize the mutual information between states and skills, $I(S; Z)$\\nSkills should be distinguishable by states, not actions. ‚Üí minimize the mutual information between actions and skills, conditioned on states $I(A; Z \\\\mid S)$\\n\\nThe objective function to maximize is as follows, where the policy entropy is also added to encourage diversity:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{F}(\\\\theta) \\n&= I(S; Z) + H[A \\\\mid S] - I(A; Z \\\\mid S) &  \\\\\\\\\\n&= (H(Z) - H(Z \\\\mid S)) + H[A \\\\mid S] - (H[A\\\\mid S] - H[A\\\\mid S, Z]) & \\\\\\\\\\n&= H[A\\\\mid S, Z] \\\\color{green}{- H(Z \\\\mid S) + H(Z)} & \\\\\\\\\\n&= H[A\\\\mid S, Z] + \\\\mathbb{E}_{z\\\\sim p(z), s\\\\sim\\\\rho(s)}[\\\\log p(z \\\\mid s)] - \\\\mathbb{E}_{z\\\\sim p(z)}[\\\\log p(z)] & \\\\scriptstyle{\\\\text{; can infer skills from states & p(z) is diverse.}} \\\\\\\\\\n&\\\\ge H[A\\\\mid S, Z] + \\\\mathbb{E}_{z\\\\sim p(z), s\\\\sim\\\\rho(s)}[\\\\color{red}{\\\\log D_\\\\phi(z \\\\mid s) - \\\\log p(z)}] & \\\\scriptstyle{\\\\text{; according to Jensen\\'s inequality; \"pseudo-reward\" in red.}}\\n\\\\end{aligned}\\n$$\\n\\nwhere $I(.)$ is mutual information and $H[.]$ is entropy measure. We cannot integrate all states to compute $p(z \\\\mid s)$, so approximate it with $D_\\\\phi(z \\\\mid s)$ — that is the diversity-driven discriminator function.\\n\\n\\nDIAYN Algorithm. (Image source: Eysenbach et al., 2019)\\n\\nOnce the discriminator function is learned, sampling a new MDP for training is strainght-forward: First, sample a latent variable, $z \\\\sim p(z)$ and construct a reward function $r_z(s) = \\\\log(D(z \\\\vert s))$. Pairing the reward function with a predefined CMP creates a new MDP.\\n\\n\\nCited as:\\n@article{weng2019metaRL,\\n  title   = \"Meta Reinforcement Learning\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2019\",\\n  url     = \"https://lilianweng.github.io/posts/2019-06-23-meta-rl/\"\\n}\\nReferences#\\n[1] Richard S. Sutton. “The Bitter Lesson.” March 13, 2019.\\n[2] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. “Learning to learn using gradient descent.” Intl. Conf. on Artificial Neural Networks. 2001.\\n[3] Jane X Wang, et al. “Learning to reinforcement learn.” arXiv preprint arXiv:1611.05763 (2016).\\n[4] Yan Duan, et al. “RL $^ 2$: Fast Reinforcement Learning via Slow Reinforcement Learning.” ICLR 2017.\\n[5] Matthew Botvinick, et al. “Reinforcement Learning, Fast and Slow” Cell Review, Volume 23, Issue 5, P408-422, May 01, 2019.\\n[6] Jeff Clune. “AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence” arXiv preprint arXiv:1905.10985 (2019).\\n[7] Zhongwen Xu, et al. “Meta-Gradient Reinforcement Learning” NIPS 2018.\\n[8] Rein Houthooft, et al. “Evolved Policy Gradients.” NIPS 2018.\\n[9] Tim Salimans, et al. “Evolution strategies as a scalable alternative to reinforcement learning.” arXiv preprint arXiv:1703.03864 (2017).\\n[10] Abhishek Gupta, et al. “Meta-Reinforcement Learning of Structured Exploration Strategies.” NIPS 2018.\\n[11] Alexander Pritzel, et al. “Neural episodic control.” Proc. Intl. Conf. on Machine Learning, Volume 70, 2017.\\n[12] Charles Blundell, et al. “Model-free episodic control.” arXiv preprint arXiv:1606.04460 (2016).\\n[13] Samuel Ritter, et al. “Been there, done that: Meta-learning with episodic recall.” ICML, 2018.\\n[14] Rui Wang et al. “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions” arXiv preprint arXiv:1901.01753 (2019).\\n[15] Uber Engineering Blog: “POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and their Solutions through the Paired Open-Ended Trailblazer.” Jan 8, 2019.\\n[16] Abhishek Gupta, et al.“Unsupervised meta-learning for Reinforcement Learning” arXiv preprint arXiv:1806.04640 (2018).\\n[17] Eysenbach, Benjamin, et al. “Diversity is all you need: Learning skills without a reward function.” ICLR 2019.\\n[18] Max Jaderberg, et al. “Population Based Training of Neural Networks.” arXiv preprint arXiv:1711.09846 (2017).\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the web-loaded document into chunks for embedding and retrieval\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "splits=text_splitter.split_documents(doc)\n",
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pa2SbHQjaEN",
        "outputId": "82fd7be0-aab2-4e44-cbb5-06160d513664",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Meta Reinforcement Learning\\n    \\nDate: June 23, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='In my earlier post on meta-learning, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to “meta-learn” Reinforcement Learning (RL) tasks by developing an agent that can solve unseen tasks fast and efficiently.\\nTo recap, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training. The adaptation process, essentially a mini learning session, happens at test with limited exposure to the new configurations. Even without any explicit fine-tuning (no gradient backpropagation on trainable variables), the meta-learning model autonomously adjusts internal hidden states to learn.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Training RL algorithms can be notoriously difficult sometimes. If the meta-learning agent could become so smart that the distribution of solvable unseen tasks grows extremely broad, we are on track towards general purpose methods — essentially building a “brain” which would solve all kinds of RL problems without much human interference or manual feature engineering. Sounds amazing, right? \\uf8ffüíñ\\nOn the Origin of Meta-RL#\\nBack in 2001#\\nI encountered a paper  written in 2001 by Hochreiter et al. when reading Wang et al., 2016. Although the idea was proposed for supervised learning, there are so many resemblances to the current approach to meta-RL.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The meta-learning system consists of the supervisory and the subordinate systems. The subordinate system is a recurrent neural network that takes as input both the observation at the current time step, $x\\\\_t$ and the label at the last time step, $y\\\\_{t-1}$. (Image source: Hochreiter et al., 2001)\\n\\nHochreiter’s meta-learning model is a recurrent network with LSTM cell. LSTM is a good choice because it can internalize a history of inputs and tune its own weights effectively through BPTT. The training data contains $K$ sequences and each sequence is consist of $N$ samples generated by a target function $f_k(.), k=1, \\\\dots, K$,\\n\\n$$\\n\\\\{\\\\text{input: }(\\\\mathbf{x}^k_i, \\\\mathbf{y}^k_{i-1}) \\\\to \\\\text{label: }\\\\mathbf{y}^k_i\\\\}_{i=1}^N\\n\\\\text{ where }\\\\mathbf{y}^k_i = f_k(\\\\mathbf{x}^k_i)\\n$$'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Noted that the last label $\\\\mathbf{y}^k_{i-1}$ is also provided as an auxiliary input so that the function can learn the presented mapping.\\nIn the experiment of decoding two-dimensional quadratic functions, $a x_1^2 + b x_2^2 + c x_1 x_2 + d x_1 + e x_2 + f$, with coefficients $a$-$f$ are randomly sampled from [-1, 1], this meta-learning system was able to approximate the function after seeing only ~35 examples.\\nProposal in 2016#\\nIn the modern days of DL, Wang et al. (2016) and Duan et al. (2017) simultaneously proposed the very similar idea of Meta-RL (it is called RL^2 in the second paper). A meta-RL model is trained over a distribution of MDPs, and at test time, it is able to learn to solve a new task quickly. The goal of meta-RL is ambitious, taking one step further towards general algorithms.\\nDefine Meta-RL#'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Define Meta-RL#\\nMeta Reinforcement Learning, in short, is to do meta-learning in the field of reinforcement learning. Usually the train and test tasks are different but drawn from the same family of problems; i.e., experiments in the papers included multi-armed bandit with different reward probabilities, mazes with different layouts, same robots but with different physical parameters in simulator, and many others.\\nFormulation#\\nLet’s say we have a distribution of tasks, each formularized as an MDP (Markov Decision Process), $M_i \\\\in \\\\mathcal{M}$. An MDP is determined by a 4-tuple, $M_i= \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P_i, R_i \\\\rangle$:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Symbol\\nMeaning\\n\\n\\n\\n\\n$\\\\mathcal{S}$\\nA set of states.\\n\\n\\n$\\\\mathcal{A}$\\nA set of actions.\\n\\n\\n$P_i: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{S} \\\\to \\\\mathbb{R}_{+}$\\nTransition probability function.\\n\\n\\n$R_i: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}$\\nReward function.\\n\\n\\n\\n(RL^2 paper adds an extra parameter, horizon $T$, into the MDP tuple to emphasize that each MDP should have a finite horizon.)\\nNote that common state $\\\\mathcal{S}$ and action space $\\\\mathcal{A}$ are used above, so that a (stochastic) policy: $\\\\pi_\\\\theta: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\to \\\\mathbb{R}_{+}$ would get inputs compatible across different tasks. The test tasks are sampled from the same distribution $\\\\mathcal{M}$ or slightly modified version.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content=\"Illustration of meta-RL, containing two optimization loops. The outer loop samples a new environment in every iteration and adjusts parameters that determine the agent's behavior. In the inner loop, the agent interacts with the environment and optimizes for the maximal reward. (Image source: Botvinick, et al. 2019)\\n\\nMain Differences from RL#\\nThe overall configure of meta-RL is very similar to an ordinary RL algorithm, except that the last reward $r_{t-1}$ and the last action $a_{t-1}$ are also incorporated into the policy observation in addition to the current state $s_t$.\\n\\nIn RL: $\\\\pi_\\\\theta(s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\\nIn meta-RL: $\\\\pi_\\\\theta(a_{t-1}, r_{t-1}, s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='In RL: $\\\\pi_\\\\theta(s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\\nIn meta-RL: $\\\\pi_\\\\theta(a_{t-1}, r_{t-1}, s_t) \\\\to$  a distribution over $\\\\mathcal{A}$\\n\\nThe intention of this design is to feed a history into the model so that the policy can internalize the dynamics between states, rewards, and actions in the current MDP and adjust its strategy accordingly. This is well aligned with the setup in Hochreiter’s system. Both meta-RL and RL^2 implemented an LSTM policy and the LSTM’s hidden states serve as a memory for tracking characteristics of the trajectories. Because the policy is recurrent, there is no need to feed the last state as inputs explicitly.\\nThe training procedure works as follows:\\n\\nSample a new MDP, $M_i \\\\sim \\\\mathcal{M}$;\\nReset the hidden state of the model;\\nCollect multiple trajectories and update the model weights;\\nRepeat from step 1.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Sample a new MDP, $M_i \\\\sim \\\\mathcal{M}$;\\nReset the hidden state of the model;\\nCollect multiple trajectories and update the model weights;\\nRepeat from step 1.\\n\\n\\n\\nIn the meta-RL paper, different actor-critic architectures all use a recurrent model. Last reward and last action are additional inputs. The observation is fed into the LSTM either as a one-hot vector or as an embedding vector after passed through an encoder model. (Image source: Wang et al., 2016)\\n\\n\\n\\nAs described in the RL^2 paper, illustration of the procedure of the model interacting with a series of MDPs in training time . (Image source: Duan et al., 2017)\\n\\nKey Components#\\nThere are three key components in Meta-RL:\\n\\n‚≠ê A Model with Memory\\n\\nA recurrent neural network maintains a hidden state. Thus, it could acquire and memorize the knowledge about the current task by updating the hidden state during rollouts. Without memory, meta-RL would not work.\\n\\n\\n‚≠ê Meta-learning Algorithm'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='‚≠ê Meta-learning Algorithm\\n\\nA meta-learning algorithm refers to how we can update the model weights to optimize for the purpose of solving an unseen task fast at test time. In both Meta-RL and RL^2 papers, the meta-learning algorithm is the ordinary gradient descent update of LSTM with hidden state reset between a switch of MDPs.\\n\\n\\n‚≠ê A Distribution of MDPs\\n\\nWhile the agent is exposed to a variety of environments and tasks during training, it has to learn how to adapt to different MDPs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='According to Botvinick et al. (2019), one source of slowness in RL training is weak inductive bias ( = “a set of assumptions that the learner uses to predict outputs given inputs that it has not encountered”). As a general ML rule, a learning algorithm with weak inductive bias will be able to master a wider range of variance, but usually, will be less sample-efficient. Therefore, to narrow down the hypotheses with stronger inductive biases help improve the learning speed.\\nIn meta-RL, we impose certain types of inductive biases from the task distribution and store them in memory. Which inductive bias to adopt at test time depends on the algorithm. Together, these three key components depict a compelling view of meta-RL: Adjusting the weights of a recurrent network is slow but it allows the model to work out a new task fast with its own RL algorithm implemented in its internal activity dynamics.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Meta-RL interestingly and not very surprisingly matches the ideas in the AI-GAs (“AI-Generating Algorithms”) paper by Jeff Clune (2019). He proposed that one efficient way towards building general AI is to make learning as automatic as possible. The AI-GAs approach involves three pillars: (1) meta-learning architectures, (2) meta-learning algorithms, and (3) automatically generated environments for effective learning.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The topic of designing good recurrent network architectures is a bit too broad to be discussed here, so I will skip it. Next, let’s look further into another two components: meta-learning algorithms in the context of meta-RL and how to acquire a variety of training MDPs.\\nMeta-Learning Algorithms for Meta-RL#\\nMy previous post on meta-learning has covered several classic meta-learning algorithms. Here I’m gonna include more related to RL.\\nOptimizing Model Weights for Meta-learning#\\nBoth MAML (Finn, et al. 2017) and Reptile (Nichol et al., 2018) are methods on updating model parameters in order to achieve good generalization performance on new tasks. See an earlier post section on MAML and Reptile.\\nMeta-learning Hyperparameters#\\nThe return function in an RL problem, $G_t^{(n)}$ or $G_t^\\\\lambda$, involves a few hyperparameters that are often set heuristically, like the discount factor $\\\\gamma$ and the bootstrapping parameter $\\\\lambda$.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Meta-gradient RL (Xu et al., 2018) considers them as meta-parameters, $\\\\eta=\\\\{\\\\gamma, \\\\lambda \\\\}$, that can be tuned and learned online while an agent is interacting with the environment. Therefore, the return becomes a function of $\\\\eta$ and dynamically adapts itself to a specific task over time.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='$$\\n\\\\begin{aligned}\\nG_\\\\eta^{(n)}(\\\\tau_t) &= R_{t+1} + \\\\gamma R_{t+2} + \\\\dots + \\\\gamma^{n-1}R_{t+n} + \\\\gamma^n v_\\\\theta(s_{t+n}) & \\\\scriptstyle{\\\\text{; n-step return}} \\\\\\\\\\nG_\\\\eta^{\\\\lambda}(\\\\tau_t) &= (1-\\\\lambda) \\\\sum_{n=1}^\\\\infty \\\\lambda^{n-1} G_\\\\eta^{(n)} & \\\\scriptstyle{\\\\text{; Œª-return, mixture of n-step returns}}\\n\\\\end{aligned}\\n$$\\n\\nDuring training, we would like to update the policy parameters with gradients as a function of all the information in hand, $\\\\theta’ = \\\\theta + f(\\\\tau, \\\\theta, \\\\eta)$, where $\\\\theta$ are the current model weights, $\\\\tau$ is a sequence of trajectories, and $\\\\eta$ are the meta-parameters.\\nMeanwhile, let’s say we have a meta-objective function $J(\\\\tau, \\\\theta, \\\\eta)$ as a performance measure. The training process follows the principle of online cross-validation, using a sequence of consecutive experiences:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Starting with parameter $\\\\theta$, the policy $\\\\pi_\\\\theta$ is updated on the first batch of samples $\\\\tau$, resulting in $\\\\theta’$.\\nThen we continue running the policy $\\\\pi_{\\\\theta’}$ to collect a new set of experiences $\\\\tau’$, just following $\\\\tau$ consecutively in time. The performance is measured as $J(\\\\tau’, \\\\theta’, \\\\bar{\\\\eta})$ with a fixed meta-parameter $\\\\bar{\\\\eta}$.\\nThe gradient of meta-objective $J(\\\\tau’, \\\\theta’, \\\\bar{\\\\eta})$ w.r.t. $\\\\eta$ is used to update $\\\\eta$:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content=\"$$\\n\\\\begin{aligned}\\n\\\\Delta \\\\eta\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\eta} \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\theta'} \\\\frac{d\\\\theta'}{d\\\\eta} & \\\\scriptstyle{\\\\text{ ; single variable chain rule.}} \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\theta'} \\\\frac{\\\\partial (\\\\theta + f(\\\\tau, \\\\theta, \\\\eta))}{\\\\partial\\\\eta}  \\\\\\\\\\n&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\theta'} \\\\Big(\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\theta}\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\\\frac{d\\\\eta}{d\\\\eta} \\\\Big) & \\\\scriptstyle{\\\\text{; multivariable chain rule.}}\\\\\\\\\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content=\"&= -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\theta'} \\\\Big( \\\\color{red}{\\\\big(\\\\mathbf{I} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\theta}\\\\big)}\\\\frac{d\\\\theta}{d\\\\eta} + \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\\\Big) & \\\\scriptstyle{\\\\text{; secondary gradient term in red.}}\\n\\\\end{aligned}\\n$$\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content=\"where $\\\\beta$ is the learning rate for $\\\\eta$.\\nThe meta-gradient RL algorithm simplifies the computation by setting the secondary gradient term to zero, $\\\\mathbf{I} + \\\\partial g(\\\\tau, \\\\theta, \\\\eta)/\\\\partial\\\\theta = 0$ — this choice prefers the immediate effect of the meta-parameters $\\\\eta$ on the parameters $\\\\theta$. Eventually we get:\\n\\n$$\\n\\\\Delta \\\\eta = -\\\\beta \\\\frac{\\\\partial J(\\\\tau', \\\\theta', \\\\bar{\\\\eta})}{\\\\partial \\\\theta'} \\\\frac{\\\\partial f(\\\\tau, \\\\theta, \\\\eta)}{\\\\partial\\\\eta}\\n$$\\n\\nExperiments in the paper adopted the meta-objective function same as $TD(\\\\lambda)$ algorithm, minimizing the error between the approximated value function $v_\\\\theta(s)$ and the $\\\\lambda$-return:\\n\\n$$\\n\\\\begin{aligned}\\nJ(\\\\tau, \\\\theta, \\\\eta) &= (G^\\\\lambda_\\\\eta(\\\\tau) - v_\\\\theta(s))^2 \\\\\\\\\\nJ(\\\\tau', \\\\theta', \\\\bar{\\\\eta}) &= (G^\\\\lambda_{\\\\bar{\\\\eta}}(\\\\tau') - v_{\\\\theta'}(s'))^2\\n\\\\end{aligned}\\n$$\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content=\"$$\\n\\\\begin{aligned}\\nJ(\\\\tau, \\\\theta, \\\\eta) &= (G^\\\\lambda_\\\\eta(\\\\tau) - v_\\\\theta(s))^2 \\\\\\\\\\nJ(\\\\tau', \\\\theta', \\\\bar{\\\\eta}) &= (G^\\\\lambda_{\\\\bar{\\\\eta}}(\\\\tau') - v_{\\\\theta'}(s'))^2\\n\\\\end{aligned}\\n$$\\n\\nMeta-learning the Loss Function#\\nIn policy gradient algorithms, the expected total reward is maximized by updating the policy parameters $\\\\theta$ in the direction of estimated gradient (Schulman et al., 2016),\\n\\n$$\\ng = \\\\mathbb{E}[\\\\sum_{t=0}^\\\\infty \\\\Psi_t \\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta (a_t \\\\mid s_t)]\\n$$\\n\\nwhere the candidates for $\\\\Psi_t$ include the trajectory return $G_t$, the Q value $Q(s_t, a_t)$, or the advantage value $A(s_t, a_t)$. The corresponding surrogate loss function for the policy gradient can be reverse-engineered:\\n\\n$$\\nL_\\\\text{pg} = \\\\mathbb{E}[\\\\sum_{t=0}^\\\\infty \\\\Psi_t \\\\log \\\\pi_\\\\theta (a_t \\\\mid s_t)]\\n$$\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='$$\\nL_\\\\text{pg} = \\\\mathbb{E}[\\\\sum_{t=0}^\\\\infty \\\\Psi_t \\\\log \\\\pi_\\\\theta (a_t \\\\mid s_t)]\\n$$\\n\\nThis loss function is a measure over a history of trajectories, $(s_0, a_0, r_0, \\\\dots, s_t, a_t, r_t, \\\\dots)$. Evolved Policy Gradient (EPG; Houthooft, et al, 2018) takes a step further by defining the policy gradient loss function as a temporal convolution (1-D convolution) over the agent’s past experience, $L_\\\\phi$. The parameters $\\\\phi$ of the loss function network are evolved in a way that an agent can achieve higher returns.\\nSimilar to many meta-learning algorithms, EPG has two optimization loops:\\n\\nIn the internal loop, an agent learns to improve its policy $\\\\pi_\\\\theta$.\\nIn the outer loop, the model updates the parameters $\\\\phi$ of the loss function $L_\\\\phi$. Because there is no explicit way to write down a differentiable equation between the return and the loss, EPG turned to Evolutionary Strategies (ES).'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='A general idea is to train a population of $N$ agents, each of them is trained with the loss function $L_{\\\\phi + \\\\sigma \\\\epsilon_i}$ parameterized with $\\\\phi$ added with a small Gaussian noise $\\\\epsilon_i \\\\sim \\\\mathcal{N}(0, \\\\mathbf{I})$ of standard deviation $\\\\sigma$. During the inner loop’s training, EPG tracks a history of experience and updates the policy parameters according to the loss function $L_{\\\\phi + \\\\sigma\\\\epsilon_i}$ for each agent:\\n\\n$$\\n\\\\theta_i \\\\leftarrow \\\\theta - \\\\alpha_\\\\text{in} \\\\nabla_\\\\theta L_{\\\\phi + \\\\sigma \\\\epsilon_i} (\\\\pi_\\\\theta, \\\\tau_{t-K, \\\\dots, t})\\n$$'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='$$\\n\\\\theta_i \\\\leftarrow \\\\theta - \\\\alpha_\\\\text{in} \\\\nabla_\\\\theta L_{\\\\phi + \\\\sigma \\\\epsilon_i} (\\\\pi_\\\\theta, \\\\tau_{t-K, \\\\dots, t})\\n$$\\n\\nwhere $\\\\alpha_\\\\text{in}$ is the learning rate of the inner loop and $\\\\tau_{t-K, \\\\dots, t}$ is a sequence of $M$ transitions up to the current time step $t$.\\nOnce the inner loop policy is mature enough, the policy is evaluated by the mean return $\\\\bar{G}_{\\\\phi+\\\\sigma\\\\epsilon_i}$ over multiple randomly sampled trajectories. Eventually, we are able to estimate the gradient of $\\\\phi$ according to NES numerically (Salimans et al, 2017). While repeating this process, both the policy parameters $\\\\theta$ and the loss function weights $\\\\phi$ are being updated simultaneously to achieve higher returns.\\n\\n$$\\n\\\\phi \\\\leftarrow \\\\phi + \\\\alpha_\\\\text{out} \\\\frac{1}{\\\\sigma N} \\\\sum_{i=1}^N \\\\epsilon_i G_{\\\\phi+\\\\sigma\\\\epsilon_i}\\n$$'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='where $\\\\alpha_\\\\text{out}$ is the learning rate of the outer loop.\\nIn practice, the loss $L_\\\\phi$ is bootstrapped with an ordinary policy gradient (such as REINFORCE or PPO) surrogate loss $L_\\\\text{pg}$, $\\\\hat{L} = (1-\\\\alpha) L_\\\\phi + \\\\alpha L_\\\\text{pg}$. The weight $\\\\alpha$ is annealing from 1 to 0 gradually during training. At test time, the loss function parameter $\\\\phi$ stays fixed and the loss value is computed over a history of experience to update the policy parameters $\\\\theta$.\\nMeta-learning the Exploration Strategies#\\nThe exploitation vs exploration dilemma is a critical problem in RL. Common ways to do exploration include $\\\\epsilon$-greedy, random noise on actions, or stochastic policy with built-in randomness on the action space.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='MAESN (Gupta et al, 2018) is an algorithm to learn structured action noise from prior experience for better and more effective exploration. Simply adding random noise on actions cannot capture task-dependent or time-correlated exploration strategies. MAESN changes the policy to condition on a per-task random variable $z_i \\\\sim \\\\mathcal{N}(\\\\mu_i, \\\\sigma_i)$, for $i$-th task $M_i$, so we would have a policy $a \\\\sim \\\\pi_\\\\theta(a\\\\mid s, z_i)$.\\nThe latent variable $z_i$ is sampled once and fixed during one episode. Intuitively, the latent variable determines one type of behavior (or skills) that should be explored more at the beginning of a rollout and the agent would adjust its actions accordingly. Both the policy parameters and latent space are optimized to maximize the total task rewards. In the meantime, the policy learns to make use of the latent variables for exploration.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='In addition,  the loss function includes a KL divergence between the learned latent variable and a unit Gaussian prior, $D_\\\\text{KL}(\\\\mathcal{N}(\\\\mu_i, \\\\sigma_i)|\\\\mathcal{N}(0, \\\\mathbf{I}))$. On one hand, it restricts the learned latent space not too far from a common prior. On the other hand, it creates the variational evidence lower bound (ELBO) for the reward function. Interestingly the paper found that $(\\\\mu_i, \\\\sigma_i)$ for each task are usually close to the prior at convergence.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The policy is conditioned on a latent variable variable $z\\\\_i \\\\sim \\\\mathcal{N}(\\\\mu, \\\\sigma)$ that is sampled once every episode. Each task has different hyperparameters for the latent variable distribution, $(\\\\mu\\\\_i, \\\\sigma\\\\_i)$ and they are optimized in the outer loop. (Image source: Gupta et al, 2018)'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Episodic Control#\\nA major criticism of RL is on its sample inefficiency. A large number of samples and small learning steps are required for incremental parameter adjustment in RL in order to maximize generalization and avoid catastrophic forgetting of earlier learning (Botvinick et al., 2019).\\nEpisodic control (Lengyel & Dayan, 2008) is proposed as a solution to avoid forgetting and improve generalization while training at a faster speed. It is partially inspired by hypotheses on instance-based hippocampal learning.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='An episodic memory keeps explicit records of past events and uses these records directly as point of reference for making new decisions (i.e. just like metric-based meta-learning). In MFEC (Model-Free Episodic Control; Blundell et al., 2016), the memory is modeled as a big table, storing the state-action pair $(s, a)$ as key and the corresponding Q-value $Q_\\\\text{EC}(s, a)$ as value. When receiving a new observation $s$, the Q value is estimated in an non-parametric way as the average Q-value of top $k$ most similar samples:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='$$\\n\\\\hat{Q}_\\\\text{EC}(s, a) = \\n\\\\begin{cases}\\nQ_\\\\text{EC}(s, a)                      & \\\\text{if } (s,a) \\\\in Q_\\\\text{EC}, \\\\\\\\\\n\\\\frac{1}{k} \\\\sum_{i=1}^k Q(s^{(i)}, a) & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\nwhere $s^{(i)}, i=1, \\\\dots, k$ are top $k$ states with smallest distances to the state $s$. Then the action that yields the highest estimated Q value is selected. Then the memory table is updated according to the return received at $s_t$:\\n\\n$$\\nQ_\\\\text{EC}(s, a) \\\\leftarrow\\n\\\\begin{cases}\\n\\\\max\\\\{Q_\\\\text{EC}(s_t, a_t), G_t\\\\}  & \\\\text{if } (s,a) \\\\in Q_\\\\text{EC}, \\\\\\\\\\nG_t                                 & \\\\text{otherwise}\\n\\\\end{cases}\\n$$'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='$$\\nQ_\\\\text{EC}(s, a) \\\\leftarrow\\n\\\\begin{cases}\\n\\\\max\\\\{Q_\\\\text{EC}(s_t, a_t), G_t\\\\}  & \\\\text{if } (s,a) \\\\in Q_\\\\text{EC}, \\\\\\\\\\nG_t                                 & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\nAs a tabular RL method, MFEC suffers from large memory consumption and a lack of ways to generalize among similar states. The first one can be fixed with an LRU cache. Inspired by metric-based meta-learning, especially Matching Networks (Vinyals et al., 2016), the generalization problem is improved in a follow-up algorithm, NEC (Neural Episodic Control; Pritzel et al., 2016).\\nThe episodic memory in NEC is a Differentiable Neural Dictionary (DND), where the key is a convolutional embedding vector of input image pixels and the value stores estimated Q value. Given an inquiry key, the output is a weighted sum of values of top similar keys, where the weight is a normalized kernel measure between the query key and the selected key in the dictionary. This sounds like a hard attention machanism.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Fig. 6 Illustrations of episodic memory module in NEC and two operations on a differentiable neural dictionary. (Image source: Pritzel et al., 2016)\\n\\nFurther, Episodic LSTM (Ritter et al., 2018) enhances the basic LSTM architecture with a DND episodic memory, which stores task context embeddings as keys and the LSTM cell states as values. The stored hidden states are retrieved and added directly to the current cell state through the same gating mechanism within LSTM:\\n\\n\\nIllustration of the episodic LSTM architecture. The additional structure of episodic memory is in bold. (Image source: Ritter et al., 2018)'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Illustration of the episodic LSTM architecture. The additional structure of episodic memory is in bold. (Image source: Ritter et al., 2018)\\n\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{c}_t &= \\\\mathbf{i}_t \\\\circ \\\\mathbf{c}_\\\\text{in} + \\\\mathbf{f}_t \\\\circ \\\\mathbf{c}_{t-1} + \\\\color{green}{\\\\mathbf{r}_t \\\\circ \\\\mathbf{c}_\\\\text{ep}} &\\\\\\\\\\n\\\\mathbf{i}_t &= \\\\sigma(\\\\mathbf{W}_{i} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_i) & \\\\scriptstyle{\\\\text{; input gate}} \\\\\\\\\\n\\\\mathbf{f}_t &= \\\\sigma(\\\\mathbf{W}_{f} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_f) & \\\\scriptstyle{\\\\text{; forget gate}} \\\\\\\\\\n\\\\color{green}{\\\\mathbf{r}_t} & \\\\color{green}{=} \\\\color{green}{\\\\sigma(\\\\mathbf{W}_{r} \\\\cdot [\\\\mathbf{h}_{t-1}, \\\\mathbf{x}_t] + \\\\mathbf{b}_r)} & \\\\scriptstyle{\\\\text{; reinstatement gate}}\\n\\\\end{aligned}\\n$$'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='where $\\\\mathbf{c}_t$ and $\\\\mathbf{h}_t$ are hidden and cell state at time $t$; $\\\\mathbf{i}_t$, $\\\\mathbf{f}_t$ and $\\\\mathbf{r}_t$ are input, forget and reinstatement gates, respectively; $\\\\mathbf{c}_\\\\text{ep}$ is the retrieved cell state from episodic memory. The newly added episodic memory components are marked in green.\\nThis architecture provides a shortcut to the prior experience through context-based retrieval. Meanwhile, explicitly saving the task-dependent experience in an external memory avoids forgetting. In the paper, all the experiments have manually designed context vectors. How to construct an effective and efficient format of task context embeddings for more free-formed tasks would be an interesting topic.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Overall the capacity of episodic control is limited by the complexity of the environment. It is very rare for an agent to repeatedly visit exactly the same states in a real-world task, so properly encoding the states is critical. The learned embedding space compresses the observation data into a lower dimension space and, in the meantime, two states being close in this space are expected to demand similar strategies.\\nTraining Task Acquisition#\\nAmong three key components, how to design a proper distribution of tasks is the less studied and probably the most specific one to meta-RL itself. As described above, each task is a MDP: $M_i = \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P_i, R_i \\\\rangle \\\\in \\\\mathcal{M}$. We can build a distribution of MDPs by modifying:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The reward configuration: Among different tasks, same behavior might get rewarded differently according to $R_i$.\\nOr, the environment: The transition function $P_i$ can be reshaped by initializing the environment with varying shifts between states.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Task Generation by Domain Randomization#\\nRandomizing parameters in a simulator is an easy way to obtain tasks with modified transition functions. If interested in learning further, check my last post on domain randomization.\\nEvolutionary Algorithm on Environment Generation#\\nEvolutionary algorithm is a gradient-free heuristic-based optimization method, inspired by natural selection. A population of solutions follows a loop of evaluation, selection, reproduction, and mutation. Eventually, good solutions survive and thus get selected.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='POET (Wang et al, 2019), a framework based on the evolutionary algorithm, attempts to generate tasks while the problems themselves are being solved. The implementation of POET is only specifically designed for a simple 2D bipedal walker environment but points out an interesting direction. It is noteworthy that the evolutionary algorithm has had some compelling applications in Deep Learning like EPG and PBT (Population-Based Training;  Jaderberg et al, 2017).'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='An example bipedal walking environment (top) and an overview of POET (bottom). (Image source: POET blog post)\\n\\nThe 2D bipedal walking environment is evolving: from a simple flat surface to a much more difficult trail with potential gaps, stumps, and rough terrains. POET pairs the generation of environmental challenges and the optimization of agents together so as to (a) select agents that can resolve current challenges and (b) evolve environments to be solvable. The algorithm maintains a list of environment-agent pairs and repeats the following:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Mutation: Generate new environments from currently active environments. Note that here types of mutation operations are created just for bipedal walker and a new environment would demand a new set of configurations.\\nOptimization: Train paired agents within their respective environments.\\nSelection: Periodically attempt to transfer current agents from one environment to another. Copy and update the best performing agent for every environment. The intuition is that skills learned in one environment might be helpful for a different environment.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The procedure above is quite similar to PBT, but PBT mutates and evolves hyperparameters instead. To some extent, POET is doing domain randomization, as all the gaps, stumps and terrain roughness are controlled by some randomization probability parameters. Different from DR, the agents are not exposed to a fully randomized difficult environment all at once, but instead they are learning gradually with a curriculum configured by the evolutionary algorithm.\\nLearning with Random Rewards#\\nAn MDP without a reward function $R$ is known as a Controlled Markov process (CMP). Given a predefined CMP, $\\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P\\\\rangle$, we can acquire a variety of tasks by generating a collection of reward functions $\\\\mathcal{R}$ that encourage the training of an effective meta-learning policy.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Gupta et al. (2018) proposed two unsupervised approaches  for growing the task distribution in the context of CMP. Assuming there is an underlying latent variable $z \\\\sim p(z)$ associated with every task, it parameterizes/determines a reward function: $r_z(s) = \\\\log D(z|s)$, where a “discriminator” function $D(.)$ is used to extract the latent variable from the state. The paper described two ways to construct a discriminator function:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Sample random weights $\\\\phi_\\\\text{rand}$ of the discriminator, $D_{\\\\phi_\\\\text{rand}}(z \\\\mid s)$.\\nLearn a discriminator function to encourage diversity-driven exploration. This method is introduced in more details in another sister paper “DIAYN” (Eysenbach et al., 2018).\\n\\nDIAYN, short for “Diversity is all you need”, is a framework to encourage a policy to learn useful skills without a reward function. It explicitly models the latent variable $z$ as a skill embedding and makes the policy conditioned on $z$ in addition to state $s$, $\\\\pi_\\\\theta(a \\\\mid s, z)$. (Ok, this part is same as MAESN unsurprisingly, as the papers are from the same group.) The design of DIAYN is motivated by a few hypotheses:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Skills should be diverse and lead to visitations of different states. ‚Üí maximize the mutual information between states and skills, $I(S; Z)$\\nSkills should be distinguishable by states, not actions. ‚Üí minimize the mutual information between actions and skills, conditioned on states $I(A; Z \\\\mid S)$\\n\\nThe objective function to maximize is as follows, where the policy entropy is also added to encourage diversity:'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The objective function to maximize is as follows, where the policy entropy is also added to encourage diversity:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{F}(\\\\theta) \\n&= I(S; Z) + H[A \\\\mid S] - I(A; Z \\\\mid S) &  \\\\\\\\\\n&= (H(Z) - H(Z \\\\mid S)) + H[A \\\\mid S] - (H[A\\\\mid S] - H[A\\\\mid S, Z]) & \\\\\\\\\\n&= H[A\\\\mid S, Z] \\\\color{green}{- H(Z \\\\mid S) + H(Z)} & \\\\\\\\\\n&= H[A\\\\mid S, Z] + \\\\mathbb{E}_{z\\\\sim p(z), s\\\\sim\\\\rho(s)}[\\\\log p(z \\\\mid s)] - \\\\mathbb{E}_{z\\\\sim p(z)}[\\\\log p(z)] & \\\\scriptstyle{\\\\text{; can infer skills from states & p(z) is diverse.}} \\\\\\\\\\n&\\\\ge H[A\\\\mid S, Z] + \\\\mathbb{E}_{z\\\\sim p(z), s\\\\sim\\\\rho(s)}[\\\\color{red}{\\\\log D_\\\\phi(z \\\\mid s) - \\\\log p(z)}] & \\\\scriptstyle{\\\\text{; according to Jensen\\'s inequality; \"pseudo-reward\" in red.}}\\n\\\\end{aligned}\\n$$\\n\\nwhere $I(.)$ is mutual information and $H[.]$ is entropy measure. We cannot integrate all states to compute $p(z \\\\mid s)$, so approximate it with $D_\\\\phi(z \\\\mid s)$ — that is the diversity-driven discriminator function.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='DIAYN Algorithm. (Image source: Eysenbach et al., 2019)\\n\\nOnce the discriminator function is learned, sampling a new MDP for training is strainght-forward: First, sample a latent variable, $z \\\\sim p(z)$ and construct a reward function $r_z(s) = \\\\log(D(z \\\\vert s))$. Pairing the reward function with a predefined CMP creates a new MDP.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Cited as:\\n@article{weng2019metaRL,\\n  title   = \"Meta Reinforcement Learning\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2019\",\\n  url     = \"https://lilianweng.github.io/posts/2019-06-23-meta-rl/\"\\n}\\nReferences#\\n[1] Richard S. Sutton. “The Bitter Lesson.” March 13, 2019.\\n[2] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. “Learning to learn using gradient descent.” Intl. Conf. on Artificial Neural Networks. 2001.\\n[3] Jane X Wang, et al. “Learning to reinforcement learn.” arXiv preprint arXiv:1611.05763 (2016).\\n[4] Yan Duan, et al. “RL $^ 2$: Fast Reinforcement Learning via Slow Reinforcement Learning.” ICLR 2017.\\n[5] Matthew Botvinick, et al. “Reinforcement Learning, Fast and Slow” Cell Review, Volume 23, Issue 5, P408-422, May 01, 2019.\\n[6] Jeff Clune. “AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence” arXiv preprint arXiv:1905.10985 (2019).'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='[6] Jeff Clune. “AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence” arXiv preprint arXiv:1905.10985 (2019).\\n[7] Zhongwen Xu, et al. “Meta-Gradient Reinforcement Learning” NIPS 2018.\\n[8] Rein Houthooft, et al. “Evolved Policy Gradients.” NIPS 2018.\\n[9] Tim Salimans, et al. “Evolution strategies as a scalable alternative to reinforcement learning.” arXiv preprint arXiv:1703.03864 (2017).\\n[10] Abhishek Gupta, et al. “Meta-Reinforcement Learning of Structured Exploration Strategies.” NIPS 2018.\\n[11] Alexander Pritzel, et al. “Neural episodic control.” Proc. Intl. Conf. on Machine Learning, Volume 70, 2017.\\n[12] Charles Blundell, et al. “Model-free episodic control.” arXiv preprint arXiv:1606.04460 (2016).\\n[13] Samuel Ritter, et al. “Been there, done that: Meta-learning with episodic recall.” ICML, 2018.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='[13] Samuel Ritter, et al. “Been there, done that: Meta-learning with episodic recall.” ICML, 2018.\\n[14] Rui Wang et al. “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions” arXiv preprint arXiv:1901.01753 (2019).\\n[15] Uber Engineering Blog: “POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and their Solutions through the Paired Open-Ended Trailblazer.” Jan 8, 2019.\\n[16] Abhishek Gupta, et al.“Unsupervised meta-learning for Reinforcement Learning” arXiv preprint arXiv:1806.04640 (2018).\\n[17] Eysenbach, Benjamin, et al. “Diversity is all you need: Learning skills without a reward function.” ICLR 2019.\\n[18] Max Jaderberg, et al. “Population Based Training of Neural Networks.” arXiv preprint arXiv:1711.09846 (2017).')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the Chroma vector store for the split documents and embeddings using Gemini\n",
        "\n",
        "vectorStore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=gemini_embeddings,\n",
        ")\n",
        "vectorStore"
      ],
      "metadata": {
        "id": "7ehGbwSMDOHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe22301-d77c-4c9d-9415-faf5b989ea60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x782b2052a6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vectorStore.as_retriever() #retrieve from vector database\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXRCXDHSjecn",
        "outputId": "80d86318-b132-4a5b-b80b-c652b81ce51e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x782b2052a6d0>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Giving instructions to guide how the language  model should behave during RAG interaction.\n",
        "\n",
        "system_prompt=(\n",
        "    \"You will be answering the questions asked\"\n",
        "    \"Use the following pieces of retrieved context to answer the question.\"\n",
        "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ],
      "metadata": {
        "id": "LiF4GRrHkrJ1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Try to mimic human respose in generate answer\n",
        "\n",
        "chat_prompt=ChatPromptTemplate.from_messages([\n",
        "    (\"system\",system_prompt),\n",
        "    (\"human\",\"{input}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "atQ2xL0Wk_Bl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answering_chain=create_stuff_documents_chain(model,chat_prompt)"
      ],
      "metadata": {
        "id": "aWmhH30BmFEF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain=create_retrieval_chain(retriever,question_answering_chain)"
      ],
      "metadata": {
        "id": "nhJCx76-l2tb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"input\":\"What are key components in Meta RL\"}) #asking question relate to the provided link"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2F11xCRmCvr",
        "outputId": "d7171a17-2aaf-40dc-9808-934f7ff20305",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What are key components in Meta RL',\n",
              " 'context': [Document(id='93cda865-969a-4a06-940b-9442bf0829ae', metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='According to Botvinick et al. (2019), one source of slowness in RL training is weak inductive bias ( = “a set of assumptions that the learner uses to predict outputs given inputs that it has not encountered”). As a general ML rule, a learning algorithm with weak inductive bias will be able to master a wider range of variance, but usually, will be less sample-efficient. Therefore, to narrow down the hypotheses with stronger inductive biases help improve the learning speed.\\nIn meta-RL, we impose certain types of inductive biases from the task distribution and store them in memory. Which inductive bias to adopt at test time depends on the algorithm. Together, these three key components depict a compelling view of meta-RL: Adjusting the weights of a recurrent network is slow but it allows the model to work out a new task fast with its own RL algorithm implemented in its internal activity dynamics.'),\n",
              "  Document(id='0c615f46-a06f-46b4-bb76-7f529e301b10', metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Overall the capacity of episodic control is limited by the complexity of the environment. It is very rare for an agent to repeatedly visit exactly the same states in a real-world task, so properly encoding the states is critical. The learned embedding space compresses the observation data into a lower dimension space and, in the meantime, two states being close in this space are expected to demand similar strategies.\\nTraining Task Acquisition#\\nAmong three key components, how to design a proper distribution of tasks is the less studied and probably the most specific one to meta-RL itself. As described above, each task is a MDP: $M_i = \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, P_i, R_i \\\\rangle \\\\in \\\\mathcal{M}$. We can build a distribution of MDPs by modifying:'),\n",
              "  Document(id='b406405b-26b6-4455-9590-94df8d01a704', metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='Sample a new MDP, $M_i \\\\sim \\\\mathcal{M}$;\\nReset the hidden state of the model;\\nCollect multiple trajectories and update the model weights;\\nRepeat from step 1.\\n\\n\\n\\nIn the meta-RL paper, different actor-critic architectures all use a recurrent model. Last reward and last action are additional inputs. The observation is fed into the LSTM either as a one-hot vector or as an embedding vector after passed through an encoder model. (Image source: Wang et al., 2016)\\n\\n\\n\\nAs described in the RL^2 paper, illustration of the procedure of the model interacting with a series of MDPs in training time . (Image source: Duan et al., 2017)\\n\\nKey Components#\\nThere are three key components in Meta-RL:\\n\\n‚≠ê A Model with Memory\\n\\nA recurrent neural network maintains a hidden state. Thus, it could acquire and memorize the knowledge about the current task by updating the hidden state during rollouts. Without memory, meta-RL would not work.\\n\\n\\n‚≠ê Meta-learning Algorithm'),\n",
              "  Document(id='ca89aa51-b9bc-4012-ae6c-13a674885faf', metadata={'source': 'https://lilianweng.github.io/posts/2019-06-23-meta-rl/'}, page_content='The topic of designing good recurrent network architectures is a bit too broad to be discussed here, so I will skip it. Next, let’s look further into another two components: meta-learning algorithms in the context of meta-RL and how to acquire a variety of training MDPs.\\nMeta-Learning Algorithms for Meta-RL#\\nMy previous post on meta-learning has covered several classic meta-learning algorithms. Here I’m gonna include more related to RL.\\nOptimizing Model Weights for Meta-learning#\\nBoth MAML (Finn, et al. 2017) and Reptile (Nichol et al., 2018) are methods on updating model parameters in order to achieve good generalization performance on new tasks. See an earlier post section on MAML and Reptile.\\nMeta-learning Hyperparameters#\\nThe return function in an RL problem, $G_t^{(n)}$ or $G_t^\\\\lambda$, involves a few hyperparameters that are often set heuristically, like the discount factor $\\\\gamma$ and the bootstrapping parameter $\\\\lambda$.')],\n",
              " 'answer': 'Here’s a breakdown of the information provided in the text, answering your questions:\\n\\n*   **According to Botvinick et al. (2019):** Weak inductive bias is a key factor in RL training, allowing for wider learning ranges but often leading to less sample efficiency. To address this, meta-RL uses inductive biases from the task distribution and stores them in memory.\\n\\n*   **In meta-RL, how to design a proper distribution of tasks is the less studied and probably the most specific one to meta-RL itself.** Each task is a MDP (Markov Decision Process), represented by the state, action, reward, and transition probabilities.\\n\\n*   **Training Task Acquisition#** The less studied and most specific one to meta-RL is designing a proper distribution of tasks.\\n\\n*   **Key Components#**\\n    *   **Model with Memory:** A recurrent neural network maintains a hidden state, allowing it to learn from past experiences.\\n    *   **Meta-learning Algorithm:** The topic of designing good recurrent network architectures is a bit too broad to be discussed here.\\n    *   **Optimizing Model Weights for Meta-learning:** Methods like MAML and Reptile are used to update model parameters for better generalization.\\n    *   **Meta-learning Hyperparameters:** Discount factor $\\\\gamma$ and bootstrapping parameter $\\\\lambda$ are key hyperparameters.\\n\\nDo you have any further questions about the text?'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}